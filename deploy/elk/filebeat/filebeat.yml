# 具体参数配置直接看官网：https://www.elastic.co/guide/en/beats/filebeat/current/configuring-howto-filebeat.html

filebeat.inputs:
- type: filestream
  id: pulsar-producer
  enabled: true
  take_over: true
  paths:
    - /var/logs/pulsar-producer/pulsar-producer-*.log  # 如果是写入有分隔，建议监听除了access.log以外的其他文件（/var/logs/access/access-*.log）
  # json配置
  parsers:
    - ndjson:
        target: ''  # 如果设置为空，就会把json全部展开，设置了字段就会是target.xxx
        add_error_key: true
        overwrite_keys: true  # 发生冲突时，解码的JSON 对象中的值将覆盖Filebeat通常添加的字段（类型、源、偏移量等）。
        expand_keys: false  # 是否展开所有json
  # 处理器(更多使用查看官网)
  processors:
    - timestamp:
        field: "log_timestamp"  # 指定包含时间字符串的字段
        target_field: "@timestamp"  # 转换后的时间戳将存储在 "@timestamp"
        layouts:
          - '2006-01-02 15:04:05.999'  # 指定时间字符串的格式,go的format格式
        timezone: "Asia/Shanghai"  # 如果指定是UTC，那么现实的timestamp会多8h;上海就是跟你的字段一样

  fields_under_root: true  # 自定义字段将作为顶级字段存储在输出文档中，而不是分组到fields子词典下。如果自定义字段名称与Filebeat添加的其他字段名称冲突，则自定义字段将覆盖其他字段。
  fields:
    from_source: pulsar-producer
    env: dev

- type: filestream
  id: pulsar-consumer
  enabled: true
  take_over: true
  paths:
    - /var/logs/pulsar-consumer/pulsar-consumer-*.log
  parsers:
    - ndjson:
        target: ''
        add_error_key: true
        overwrite_keys: true
        expand_keys: false
  processors:
    - timestamp:
        field: "log_timestamp"
        target_field: "@timestamp"
        layouts:
          - '2006-01-02 15:04:05.999'
        timezone: "Asia/Shanghai"

  fields_under_root: true
  fields:
    from_source: pulsar-consumer
    env: dev

# 定义模板的相关信息，可以确保所有索引都有一致的设置和映射。（启动以后同步的是数据流，不启动是到索引）
setup.template.enabled: false
#setup.template.name: "test_log"
#setup.template.pattern: "test-log-*"
#setup.template.overwrite: false  # 如果有就覆盖
#setup.template.settings:
#  index.number_of_shards: 1  # 我们是单节点所以就搞一个分片，一般分片数 * （副本数+1）为节点数
#  index.number_of_replicas: 0  # 副本数

# 生命周期管理
setup.ilm.enabled: false


output.elasticsearch:
  hosts: [ "http://elasticsearch:9200"]  # 事件将按循环顺序分发到这些节点。如果一个节点无法访问，事件将自动发送到另一个节点，不是每个节点都发送！！！
  username: "elastic"
  password: "zx.123"
  indices:
    - index: "dev-pulsar-consumer-%{+yyyy.MM.dd}"  # 是否需要根据时间分割%{+yyyy.MM.dd}
      when.equals:
        from_source: "pulsar-consumer"
    - index: "dev-pulsar-producer-%{+yyyy.MM.dd}"
      when.equals:
#        fields.source: "error"
        from_source: "pulsar-producer"

processors:
  - drop_fields:
      fields: ["agent.ephemeral_id", "agent.name", "agent.version", "ecs.version", "host.name", "log.file.device_id",
               "log.file.inode", "log.file.path", "log.flags", "log.offset", "agent.type", "input.type"]
